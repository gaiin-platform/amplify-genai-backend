# Amplify Chat Fargate Container Deployment

This directory contains everything needed to deploy the Amplify Chat service as a containerized application on AWS Fargate, eliminating Lambda cold starts while maintaining all existing functionality.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ALB (HTTPS)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ECS Fargate Service            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Express Server         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  (container/server.js)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ router.js           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ assistants/         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ common/llm.js       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ datasource/         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AWS Services                   ‚îÇ
‚îÇ  ‚îú‚îÄ S3 (Files)                  ‚îÇ
‚îÇ  ‚îú‚îÄ DynamoDB (State/Usage)      ‚îÇ
‚îÇ  ‚îú‚îÄ Secrets Manager             ‚îÇ
‚îÇ  ‚îî‚îÄ SQS (Assistant Queue)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Directory Structure

```
container/
‚îú‚îÄ‚îÄ server.js                 # Production Express server
‚îú‚îÄ‚îÄ Dockerfile               # Container definition
‚îú‚îÄ‚îÄ .dockerignore           # Docker build exclusions
‚îú‚îÄ‚îÄ README.md               # This file
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ build-and-push.sh  # Build and push to ECR
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh          # Deploy to ECS
‚îÇ   ‚îú‚îÄ‚îÄ test-local.sh      # Local Docker testing
‚îÇ   ‚îî‚îÄ‚îÄ logs.sh            # View CloudWatch logs
‚îî‚îÄ‚îÄ terraform/
    ‚îú‚îÄ‚îÄ main.tf            # Main Terraform config
    ‚îú‚îÄ‚îÄ variables.tf       # Input variables
    ‚îú‚îÄ‚îÄ outputs.tf         # Output values
    ‚îú‚îÄ‚îÄ ecs.tf            # ECS cluster, service, tasks
    ‚îú‚îÄ‚îÄ alb.tf            # Application Load Balancer
    ‚îú‚îÄ‚îÄ ecr.tf            # Container registry
    ‚îî‚îÄ‚îÄ terraform.tfvars.example
```

## üöÄ Quick Start

### Prerequisites

- AWS CLI configured with appropriate credentials
- Docker installed and running
- Terraform >= 1.0
- Access to your existing var.yml configuration files

### Step 1: Local Testing

Test the container locally before deploying:

```bash
# From project root
cd container/scripts
./test-local.sh dev 8080
```

Visit `http://localhost:8080/health` to verify it's running.

### Step 2: Build and Push to ECR

```bash
cd container/scripts
./build-and-push.sh dev v1.0.0
```

This will:
- Create ECR repository if it doesn't exist
- Build the Docker image
- Push to ECR with version tag and 'latest'

### Step 3: Deploy Infrastructure with Terraform

```bash
cd container/terraform

# Copy and configure variables
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with your values

# Initialize Terraform
terraform init

# Review changes
terraform plan

# Deploy
terraform apply
```

### Step 4: Update Frontend

After deployment, update your frontend to point to the new ALB endpoint:

```javascript
// Get the ALB DNS from Terraform output
terraform output service_endpoint

// Update your frontend config
const API_ENDPOINT = "http://<alb-dns>/chat";
```

## üîß Configuration

### Environment Variables

All environment variables from `amplify-lambda-js/serverless.yml` are replicated in the ECS task definition. Key variables:

| Variable | Description | Required |
|----------|-------------|----------|
| `PORT` | Container listen port | Yes (8080) |
| `NODE_ENV` | Environment mode | Yes (production) |
| `COGNITO_USER_POOL_ID` | Cognito User Pool | Yes |
| `COGNITO_CLIENT_ID` | Cognito Client | Yes |
| `ALLOWED_ORIGINS` | CORS origins | Yes |
| `S3_FILE_TEXT_BUCKET_NAME` | File storage bucket | Yes |
| `HASH_FILES_DYNAMO_TABLE` | File metadata table | Yes |

See `terraform/ecs.tf` for the complete list.

### Terraform Variables

Key variables to configure in `terraform.tfvars`:

```hcl
# Network
vpc_id             = "vpc-xxxxx"
private_subnet_ids = ["subnet-xxxxx", "subnet-yyyyy"]
public_subnet_ids  = ["subnet-aaaaa", "subnet-bbbbb"]

# Scaling
desired_count = 2
min_capacity  = 2
max_capacity  = 10

# Resources
task_cpu    = "1024"  # 1 vCPU
task_memory = "2048"  # 2GB
```

## üìä Monitoring

### View Logs

```bash
# Recent logs
cd container/scripts
./logs.sh dev

# Follow logs in real-time
./logs.sh dev follow
```

### CloudWatch Metrics

Monitor in AWS Console:
- ECS Service ‚Üí Metrics
- Application Load Balancer ‚Üí Monitoring
- Target Group ‚Üí Health Checks

Key metrics:
- `CPUUtilization`
- `MemoryUtilization`
- `TargetResponseTime`
- `HealthyHostCount`

### Health Checks

- Container: `http://localhost:8080/health`
- ALB: `http://<alb-dns>/health`

## üîÑ Deployment Workflow

### Updating the Service

After code changes:

```bash
# 1. Build and push new image
cd container/scripts
./build-and-push.sh dev v1.0.1

# 2. Deploy to ECS
./deploy.sh dev v1.0.1
```

The deployment process:
1. Pushes new image to ECR
2. Forces ECS service update
3. ECS launches new tasks with new image
4. Waits for health checks to pass
5. Drains connections from old tasks
6. Terminates old tasks

### Rollback

To rollback to a previous version:

```bash
# Deploy specific image tag
cd container/scripts
./deploy.sh dev v1.0.0
```

Or via AWS Console:
1. ECS ‚Üí Clusters ‚Üí Service
2. Update Service ‚Üí Force New Deployment
3. Select previous task definition revision

## üîê Security

### IAM Roles

Two roles are created:

1. **Task Execution Role**: For ECS to pull images and write logs
2. **Task Role**: Application permissions (same as Lambda)
   - Reuses existing Lambda IAM policies
   - Access to S3, DynamoDB, Secrets Manager, SQS

### Network Security

- ECS tasks in private subnets (no public IP)
- ALB in public subnets
- Security groups restrict traffic:
  - ALB ‚Üí Port 443/80 from internet
  - ECS ‚Üí Port 8080 from ALB only
  - ECS ‚Üí Outbound to AWS services

### Secrets Management

Secrets can be injected via:
1. Environment variables in task definition
2. AWS Secrets Manager (recommended)
3. Parameter Store

Example in `ecs.tf`:
```hcl
secrets = [
  {
    name      = "LLM_API_KEY"
    valueFrom = "arn:aws:secretsmanager:region:account:secret:name"
  }
]
```

## üí∞ Cost Estimation

### Fargate Costs (us-east-1)

With default configuration (2 tasks, 1 vCPU, 2GB RAM):

```
Monthly Cost Breakdown:
- vCPU:    $0.04048/hour √ó 1 √ó 2 tasks √ó 730 hours = $59.10
- Memory:  $0.004445/GB/hour √ó 2GB √ó 2 tasks √ó 730 hours = $12.97
- ALB:     $0.0225/hour √ó 730 hours = $16.43
- Data:    ~$10-50/month depending on traffic

Total: ~$100-150/month base + scaling
```

Auto-scaling will add costs during peak usage but only when needed.

### Cost vs Lambda

| Scenario | Lambda (Current) | Fargate (New) | Winner |
|----------|------------------|---------------|---------|
| Low traffic (<100k req/mo) | $50-100 | $100-150 | Lambda |
| Medium traffic (100k-1M req/mo) | $500-800 | $100-200 | Fargate |
| High traffic (>1M req/mo) | $800+ | $150-300 | Fargate |
| Need low latency | ‚ùå Cold starts | ‚úÖ Always warm | Fargate |

## üß™ Testing

### Local Docker Test

```bash
# Test with curl
curl -X POST http://localhost:8080/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello"}
    ],
    "options": {
      "model": {"id": "gpt-4"}
    }
  }'
```

### Load Testing

```bash
# Install hey (HTTP load generator)
brew install hey  # or apt-get install hey

# Test with 100 requests, 10 concurrent
hey -n 100 -c 10 \
  -H "Authorization: Bearer $TOKEN" \
  -m POST \
  -d '{"messages":[{"role":"user","content":"test"}]}' \
  http://<alb-dns>/chat
```

## üêõ Troubleshooting

### Container Won't Start

Check logs:
```bash
cd container/scripts
./logs.sh dev
```

Common issues:
- Missing environment variables
- IAM permission errors
- Network connectivity issues

### 503 Errors from ALB

Possible causes:
1. No healthy targets
   - Check: ALB ‚Üí Target Groups ‚Üí Targets
   - Fix: Verify health check endpoint returns 200

2. Tasks failing health checks
   - Check: ECS ‚Üí Service ‚Üí Events
   - Fix: Review container logs

3. Security group misconfiguration
   - Check: ECS tasks can reach ALB
   - Fix: Verify security group rules

### High Memory/CPU Usage

Scale up:
```hcl
# In terraform.tfvars
task_cpu    = "2048"  # 2 vCPU
task_memory = "4096"  # 4GB
```

Then apply:
```bash
terraform apply
```

## üîÑ Migration from Lambda

### Hybrid Approach (Recommended)

Keep both deployments during migration:

1. Deploy Fargate alongside Lambda
2. Update frontend to use Fargate for chat
3. Monitor for 1-2 weeks
4. If successful, decommission Lambda chat
5. Keep Lambda for queue/billing functions

### Frontend Update

```javascript
// Before (Lambda)
const CHAT_ENDPOINT = "https://xxx.lambda-url.us-east-1.on.aws";

// After (Fargate)
const CHAT_ENDPOINT = "http://<alb-dns>/chat";
// Or with custom domain
const CHAT_ENDPOINT = "https://chat-api.yourdomain.com/chat";
```

### Feature Parity

‚úÖ All features work identically:
- Assistants (default, mapReduce, agent, codeInterpreter, etc.)
- RAG integration
- Data sources (S3, tags, external)
- All LLM providers (OpenAI, Azure, Bedrock, Gemini)
- Streaming responses
- Billing & usage tracking
- Rate limiting
- Authentication (Cognito, API keys)

‚ùå No X-Ray tracing (in this PoC - can be added later)

## üìù Next Steps

### Production Readiness

1. **Add HTTPS**
   - Request ACM certificate
   - Set `alb_certificate_arn` in terraform.tfvars
   - Apply changes

2. **Custom Domain**
   - Create Route53 A record ‚Üí ALB
   - Update CORS origins

3. **Monitoring & Alerts**
   - Set up CloudWatch alarms
   - Configure SNS notifications
   - Add application monitoring (DataDog, New Relic, etc.)

4. **CI/CD Pipeline**
   - Automate build-and-push on git push
   - Run tests before deployment
   - Blue/green deployments

5. **Add X-Ray Tracing**
   - Add X-Ray daemon sidecar container
   - Update task definition
   - Enable in application code

## üìö Additional Resources

- [AWS Fargate Documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html)
- [ECS Task Definitions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html)
- [Application Load Balancer](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html)
- [Express.js Server-Sent Events](https://expressjs.com/)

## ü§ù Support

For issues or questions:
1. Check CloudWatch logs: `./scripts/logs.sh dev`
2. Review ECS service events in AWS Console
3. Verify terraform state: `terraform show`
4. Contact the platform team
